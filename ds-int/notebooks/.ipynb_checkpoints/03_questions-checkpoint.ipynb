{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e268443d-5a36-421b-9b6b-ae0965a8fb44",
   "metadata": {},
   "source": [
    "## 1. What method did you use to validate your modelâ€™s stability, and why did you choose such method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ca09cf-cc6b-4fff-a4c9-25c3ab311b64",
   "metadata": {},
   "source": [
    "We used *K-Fold Cross Validation* with $k = 5$ and *AUC-ROC* evaluation.\n",
    "\n",
    "Stability, balance between variance and bias and performace of a model can be estimated/adressed by dividing the training data into $k$ subsets or folds. We trains on $k - 1$ fold and we use that last fold for evaluation. This is repetaded $k$ times. The approach ensures every data point in the training set is used for both processes, and it provides a comprehensive evaluation compared to a simple train-test split.\n",
    "\n",
    "Averaging the results of the model across different folds helps us reduce the inherent variance of the data and provides a more reliable estimate of its capability to generalize on unseen data. This is crucial if we want to assess how stable the model's perfomrance will be.\n",
    "\n",
    "Details and code are available in ```section 3.1``` of the notebook ```02_modeling.ipynb```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d910d57-5a02-43dc-a506-594acb84b4d8",
   "metadata": {},
   "source": [
    "## 2. Why do we prefer to use AUC as a benchmark for this model? What about PR curve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c90cd26-f83b-47f3-b1a8-0255d66340d2",
   "metadata": {},
   "source": [
    "AUC-ROC curve is a popular metric for binary classifiers; the main reason would be its **interpretability and clarity**, which makes it very **easy to explain and understand visually**, although the metric giving a value between $[0, 1]$ can help **interpreting it as a probability** too. In general, probabilities are also **easy to grasp**, specially given that explanations concerning *False Positives* and *False Negatives* are usually convoluted. \n",
    "The metric provides a **single scalar over the range of threshold values**. In our case, we are working with credit scoring, which might make us adjust the decision for the threshold based on business requirements or risk tolerance. \n",
    "\n",
    "**AUC-ROC curve summarizes and simplifies advanced concepts** that have to do with whether we are more interested in catching (or miss). For credit scoring, we tend to consider catching most of the high risk users more important than catching also a few of false positives, but AUC-ROC curve gives us a **good, balanced view of the model performance**.\n",
    "\n",
    "On the other hand, Precision-Recall curve can be valuable, specially with imbalanced datasets like the one we've used in our case. It can be used to focus on the performance of the positive class by plotting precision (positive predictive rate) versus recall (sensitivity), because it evaluates the trade-off between them. For our case study, as we mentioned earlier, we'd have to concentrate in recall, as missing a defaulter would be more critical. \n",
    "\n",
    "- **AUC_ROC vs. PR**\n",
    "  - AUC-ROC considers both classes equally imporant.\n",
    "  - PR Curve is useful for imbalanced datasets where positive class is of greater interest.\n",
    "  - AUC-ROC provides broader view, allows for changes in the decision threshold.\n",
    "  - AUC-ROC is easily interpretable, can be visualized and understood in seconds, and gives a holistic understanding of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2703bf9f-7832-405a-9a75-50ebc5eda2c4",
   "metadata": {},
   "source": [
    "## 3. Perform a preliminary analysis of the dataset and discuss your findings on data distribution. Correlate it with the results obtained from your model, and briefly discuss any explainable features of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a1d70-b252-4d32-8395-2c0c4975fdd5",
   "metadata": {},
   "source": [
    "In general, results from our **exploratory analysis fit quite well with our predictions**, whether we look at the **distributions of attributes and their possible associations**, or we point to the **correlations**, that show statistics that **make sense when looking at the feature importance or explainable techniques** we've used. We've aimed for simplicity in this case study's exploration of the data, so we concentrated in statistics (correlations, distributions' spread and skweness, basic statistics) instead of a more descriptive analysis, for a few of reasons:\n",
    "\n",
    "- The dataset is not huge in terms of columns and yet it offers a good number of datapoints for each one.\n",
    "- Variables are quite comprehensive, and the use study's goal is clear and intuitive. Attributes don't ask for a deep domain knowledge.\n",
    "- What we were more interested in doing is to correlate what we saw in the exploratory analysis with the predictions.\n",
    "\n",
    "On **data distributions** of the main features (details in ```section 2.5.5``` of the notebook ```01_data_prep_and_EDA.ipynb```):\n",
    "\n",
    "- ```RevolvingUtilizationOfUnsecuredLines```: skewed distribution with many low values, long tail of high values.\n",
    "- ```age```: somewhat normal distribution, not far from a real world scenario.\n",
    "- ```NumberOfOpenCreditLinesAndLoans```: Distribution indicates a higher number of borrowers have between 0 and 5 open lines.\n",
    "- ```ImputedMonthlyIncome```: Skewed distribution with a long tail for higher incomes.\n",
    "- ```DebtRatio```: Right skewed, with most values being lower.\n",
    "- ```NumberRealEstateLoansOrLines```: Values around 0 to 2.\n",
    "- ```NumberOfDependents```: Right skewed, with most values being 0 or 1.\n",
    "\n",
    "**Correlations** (details in details in ```section 2.5.5``` of the notebook ```01_data_prep_and_EDA.ipynb```):\n",
    "\n",
    "- ```NumberOfOpenCreditLinesAndLoans```and ```NumberRealEstateLoansOrLines``` (0.47): this correlation makes sens and it's the strongest one; individuals who have multiple types of credit accounts (like credit cards and personal loans) are also likely to have real estate loans if they are financially active.\n",
    "- ```NumberRealEstateLoansOrLines```and ```ImputedMonthlyIncome``` (0.24): This moderate correlation is also sound, as higher income individuals are more likely to qualify for and take out real estate loans, reflecting their financial capacity to invest in property.\n",
    "- ```ImputedMonthlyIncome```and ```DebtRatio``` (-0.23): A negative correlation showing that higher income individuals are better able to manage and pay off their debts, resulting in lower debt ratios.\n",
    "- ```NumberOfDependents```and ```age``` (-0.17): Shows how life stages are typical; younger individuals are more likely to have children living at home, while older individuals are likely to have grown-up children.\n",
    "\n",
    "**Explainable Features**\n",
    "These are explained thoroughly in ```section 4``` of the notebook ```02_modeling.ipynb```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a16a5a-b71c-45e5-a327-6999b3913e15",
   "metadata": {},
   "source": [
    "## 4. Are you able to plot the probability of defaults by deciles (every 10th percentile band). What is your observation there, if any?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5674b983-c0a3-4190-9c96-c0a1089ee3de",
   "metadata": {},
   "source": [
    "Code and resulting plot are available in ```02_modeling.ipynb``` notebook, ```section 5```.\n",
    "\n",
    "The plot shows the average probability of default for each decile (every 10th percentile band). The average **probability of default increases monotonically**. As was expected, it shows that **higher deciles are associated with higher default risk** with a noticeable average probability of default starting from the fifth decile, and **sharp rise in the eighth and ninth**. Of course, this last decile shows the highest rick of default.\n",
    "\n",
    "The distribution is fairly close to a *Sigmoid distribution* of the form:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "which usually arises from **processes where the increase in probability is gradual** at first and then becomes steeper in higher deciles, indicating a logistic grow pattern. \n",
    "\n",
    "On model performance, the **plot also suggests that the model is well calibrated**, and **distinguishes effectively between different levels of risk**. Results can allow us, in theory, to segment the users into different bins so they can help us better to make decisions on credit approvals, interest rates and obviously risk managent. Segmentation could be, for instance:\n",
    "\n",
    "- *Low Risk Individuals*: Deciles 0 to 4\n",
    "- *Medium Risk Individuals*: Deciles 5 to 7\n",
    "- *High Risk Individuals*: Deciles 8 to 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a354df05-98fc-44fc-990d-d9ac8880071d",
   "metadata": {},
   "source": [
    "## 5. Are you able to craft out an XAI (Explainable AI) method to explain the result of the prediction on a single datapoint?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12533180-33d2-40ad-9c91-f388b4fb6048",
   "metadata": {},
   "source": [
    "The method **SHAP (Shapley Additive Explanations)** has been used in our case study (see ```section 5``` in the ```02_modeling.ipynb``` notebook) for **single datapoint analysis**, along with **feature importance** for overall insights. SHAP is a powerful and popular method for interpreting **how attributes in the dataset contributed to the model's output**. It's important to note that SHAP describes how an output was reached (with some limitations), but it doesn't evaluate the model. In any case, it offers great advantages:\n",
    "\n",
    "- **SHAP has a robust theoretical foundation**: SHAP values come directly from [cooperative game theory](https://en.wikipedia.org/wiki/Cooperative_game_theory) as an approach to distributing the total gain or loss amongst features, based on their contrbution to the prediction.\n",
    "- **Global and Local interpretability**: Other XAI or Feature Importance techniques are usually focused on global effect of features (*Permutation Feature Importance*, *Tree Based Feature Importance*) or just local effects (*LIME*), but SHAP is capable of offering explanations at the two levels.\n",
    "- **Decomposition**: Values decompose a prediction into the sum of the contributions of each feature making it easy to see how each feature influences the output.\n",
    "- **Bee Swarm**: The summary plot used to see the features contributions is extremely intuitive and allows for a quick understanding of the prediction and its parts.\n",
    "\n",
    "XAI can be understood as the aim to **make artificial intelligence and machine learning transparent, interpretable and understandable**. This is seldom an easy task, specially for more complex models like neural networks or fairly complicated machine learning algorithms, because we often deal with mathematical operations that only computers can do, and the [black box problem](https://en.wikipedia.org/wiki/Black_box) is always there in these cases. However, even in its difficulty, **technology can become worthless if we don't factor in accountability**, or if we are blind to the things that make it work. **For businesses, not understanding why a model works is the best way to lose the grasp on reality and put in danger the hard work data departments do**.\n",
    "\n",
    "Although various methods of Explainable AI exist, **explainability and clarity should always be part of the setup of a problem**. From **model developement** (insights from XAI methods can provide insights into what's working and what's not, the choice of algorithm is also important), to **model validation** (making sure predictions align with the domain knowledge and that no spurious correlations are present), to **deployment and monitoring** (built-in explainability provides users and stakeholders with insights to make informed decisions) and also **user interaction** (XAI can enable feedback mechanisms where users provide input on predictions' correctness), every step of a project should ideally have clarity and explainability in sight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a4f14a-c251-45f9-827d-ead5d17625d0",
   "metadata": {},
   "source": [
    "## 6. See how high you can get on the leaderboard / Final thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7eb5da-4c48-4b33-b732-c45ab82dcf8f",
   "metadata": {},
   "source": [
    "As with all projects, there are always avenues that we could explore further to improve upon the job done. We've reached an $AUC-ROC$ score of $0.792$, which would put us in the low part of the leaderboard (772). Even if the goal of this project was more centered on the process than the evaluation metric, I'm fairly sure we could improve that position quite a bit if we were to dig a bit deeper into more solutions, for instance:\n",
    "\n",
    "- **Adress class imbalance**:\n",
    "    - Target is extremely imbalanced, so any kind of resampling techniques could help. SMOTE method generates synthetic samples for the minority class to balance the dataset, at the risk of having unrealistic data and also risking overfitting.\n",
    "    - Undersampling is easy to implement, there is a loss of information and we'd risk underfitting.\n",
    "    - Class weighting needs hyperparameter tuning, but it's a good candidate for our case.\n",
    " \n",
    "- **More complex models**:\n",
    "    - WE've repeated a few times that we've aimed for simplicity during the task, but more complex ensemble methods could help in this task; LightGBM, CatBoost or XGBoost are good candidates.\n",
    "\n",
    "- **More hyperparameter tuning**:\n",
    "    - The grid space we used to run through the possibilities was quite limited. That was intentional, otherwise the code wouldn't be as runnable, as it would take much more time.\n",
    "    - We also used a randomized start of the grid search. Another option would be a more extensive search that is deterministic and goes through all options. This would mean we could be sure the optimization would be top tier, but computing time would be impossible in a local environment.\n",
    " \n",
    "- **Feature engineering and selection**:\n",
    "    - We could combine features to capture more complex relationships, like polynomial features that capture non-linear relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc9eb3-d728-4867-aa28-f6098a45c14e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
